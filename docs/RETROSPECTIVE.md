# 과제 전형 회고 (Retrospective)

> **작성일**: 2026-02-27

---

## 1. 과제 분석

과제를 받고 가장 먼저 한 일은 구현 요건의 **배경을 파악**하는 것이었습니다.

클라이언트가 요청한 목록 중 다음 두 가지를 **최우선 사항**으로 판단했습니다.

1. **챗봇 API를 통해 AI를 활용할 수 있다는 것을 증명한다**  
   — VIP Onboarding 팀의 시연 목적 자체가 이것이기 때문
2. **향후 사내 대외비 문서를 챗봇 서비스에 연결할 수 있는 확장 가능한 구조**  
   — 클라이언트가 단순 데모를 넘어 실제 도입을 기대하는 맥락

이 판단을 바탕으로 피드백이나 분석 기능보다 **인증 → 대화 생성 → 문서 학습 확장 가능성** 순서로 우선순위를 설정하고, `PRD.md`와 `DEVELOPMENT_GUIDE.md`를 먼저 작성한 뒤 프로젝트 초기 세팅을 진행했습니다.

모든 API를 구현한 이후에는 **테스트를 통과해야 다음 단계로 넘어갈 수 있도록** 스스로 제한을 두었습니다. 이를 통해 기능을 쌓아가는 과정에서 회귀 오류를 줄이고자 했습니다.

---

## 2. AI 활용 방법

과제 전반에서 AI(Claude Code)를 적극 활용했습니다.

사전에 작성해 둔 `PRD.md`와 `DEVELOPMENT_GUIDE.md`를 컨텍스트로 제공하고, 요구사항에 따라 **코드 작성 → 테스트 → 수정**의 반복 흐름을 AI에게 맡겼습니다.

그 과정에서 저는 AI가 개발을 진행하는 동안 별도로 **RAG 확장 아키텍처를 리서치**했습니다.

> **고민의 핵심**: 향후 사내 대외비 문서를 임베딩하려면 어떤 구조가 맞는가?
>
> - **방안 A**: 별도 LangChain 서버를 두고 Spring에서 프록시로 연결
> - **방안 B**: Spring AI + pgvector를 모노레포로 통합
>
> 두 방안 모두 장단점이 있었고, 결국 **방안 B(Spring AI + pgvector 모노레포)**를 채택했습니다.  
> 서버를 하나 더 운영하는 부담 없이 Spring 생태계 안에서 확장이 가능하고, 의존성만 추가하면 VectorStore 인터페이스로 자연스럽게 RAG를 붙일 수 있기 때문입니다.

그러나 테스트가 전부 통과하더라도, 자동화된 테스트만으로는 **구체적인 비즈니스 로직의 안정성을 보장하지 못한다**는 생각이 들었습니다. 그래서 MVP 완성 후 Swagger를 통해 직접 사용자 플로우를 따라가며 API 응답 결과를 손수 확인했습니다.

---

## 3. 가장 어려웠던 기능 — 챗봇 응답 API의 401 오류

가장 많은 시간과 고민을 쏟은 부분은 **챗봇 응답 API에서 반복적으로 발생한 401 오류**였습니다.

트러블슈팅 과정은 다음과 같았습니다.

| 단계 | 시도 | 결과 |
|------|------|------|
| 1 | 환경변수 `OPENAI_API_KEY`가 올바른지 확인 | 올바른 키였으나 `invalid api key` 오류 지속 |
| 2 | JWT 토큰이 제대로 주입되는지 확인 | 헤더 정의 자체가 되지 않아 인증 실패 |
| 3 | Swagger produces 설정 확인 | 하나의 엔드포인트에 `APPLICATION_JSON`과 `TEXT_EVENT_STREAM` 혼재 → Spring이 Content-Type을 결정하지 못함 |
| 4 | 비스트리밍/스트리밍 엔드포인트 분리 | **정상 동작** |

이 경험에서 얻은 교훈은 두 가지입니다.

**① 테스트의 신뢰 범위 기준 세우기**

테스트가 전부 통과했음에도 실제 API 호출에서 문제가 발생했습니다. 테스트 코드는 **설계한 대로 동작하는지**를 검증할 뿐, Swagger 연동이나 Content Negotiation처럼 환경과 설정이 맞물리는 지점의 오류는 잡아내지 못합니다. 유닛/통합 테스트가 통과해도 반드시 **실제 사용자 플로우로 검증하는 단계가 필요하다**는 것을 확인했습니다.

**② E2E 시나리오 검증의 중요성**

이론적으로 작성한 코드와 실제 HTTP 클라이언트(Swagger, curl 등) 사이의 동작 차이는 생각보다 클 수 있습니다. 특히 Content-Type 협상, 인증 토큰 전달 방식 등은 직접 시나리오를 따라가 보며 확인하는 것이 가장 확실합니다.

---

## 4. AI 활용의 한계와 개선점

과제를 진행하면서 AI 도구 활용 자체에 대한 아쉬움도 있었습니다.

**컨텍스트 관리 실패**

Claude Code에서 컨텍스트가 길어지면서 토큰 한도에 도달해 작업이 중단되는 상황이 발생했습니다. Antigravity로 이전해 이어나갔지만, 중복 작업이 발생했고 **구현 분량에 비해 토큰 소모가 과도했다**는 생각이 들었습니다.

개선할 수 있었던 점:

- **태스크를 더 작은 단위로 분리**하여 컨텍스트를 짧게 유지했어야 했습니다.
- PRD를 참조하게 했지만, 매 태스크마다 **필요한 컨텍스트만 선별적으로 제공**하면 효율이 훨씬 올라갔을 것입니다.
- AI가 코드를 작성하는 동안 **비즈니스 로직 검토와 아키텍처 설계는 직접 병행**하는 분업 구조를 더 일찍 체계화했더라면 전체 흐름이 더 안정적이었을 것입니다.

**신뢰 가능한 워크플로우 구축의 어려움**

짧은 시간 안에 AI를 활용해 신뢰할 수 있는 API를 만들어내려면, 단순히 코드를 생성시키는 것을 넘어 **검증 단계를 워크플로우에 내재화**시켜야 한다는 것을 느꼈습니다. 다음 과제에서는 "AI 코드 작성 → 자동 테스트 → E2E 수동 검증"의 체크포인트를 더 명확히 설정하고 진행할 것 같습니다.

---

## 5. 정리

| 항목 | 내용 |
|------|------|
| 가장 잘된 점 | 배경 기반 우선순위 설정으로 핵심 기능에 집중 |
| 가장 아쉬운 점 | 컨텍스트 관리 실패로 인한 토큰 낭비와 중복 작업 |
| 가장 큰 배움 | 테스트 통과 ≠ 안정성. E2E 검증은 별도 필수 단계 |
| 다음에 적용할 것 | AI 태스크 분리 + 검증 체크포인트 명시화 |
